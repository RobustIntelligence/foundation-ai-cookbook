{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb39e15-bc41-4cfb-8f78-8b00e692b68c",
   "metadata": {},
   "source": [
    "# Finetuning by adding Classification Head\n",
    "\n",
    "**This use case is an extention of [Classification_cybersecurity_descriptions](https://github.com/RobustIntelligence/foundation-ai-cookbook/blob/main/2_examples/Classification_cybersecurity_descriptions.ipynb) shown in 2_examples**\n",
    "\n",
    "For this demo, we use human-annotated datasets for cyber threat intelligence analysis from CTI-HAL to determine each excerpt of security blogs, reports etc. is associated with which MITRE ATT&CK ID.  **The dataset is NOT used for training of Foundation-Sec-8B model.**\n",
    "\n",
    "To see the details of datasets, refer to\n",
    "- Paper: https://arxiv.org/abs/2504.05866 <br>\n",
    "- GitHub: https://github.com/dessertlab/CTI-HAL\n",
    "\n",
    "We'll finetune Foundation-Sec-8B as well as original llama model to show how finetuning works, and how Foundation-Sec-8B outperforms the original model.\n",
    "\n",
    "### Hardware\n",
    "This finetuning has been conducted under Nvidia 8xA100 (80GB) GPUs. Though it's doable with 1 GPU, it'll be slower. If you don't have enough memories, consider enabling QLoRa. That'll save memories at the cost of small performance degration.\n",
    "\n",
    "**Caution: The dataset used for finetuning is too small to perform effectively enough. If you are planning to implement finetuning with actual use cases, consider to use bigger datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0554f0-1a5c-432c-a2e7-ac8b74066af5",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f2b59c-3029-41f7-a108-5ff15eaaccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b26678-8c99-4a72-ab32-7af3be51be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ccffb8-092e-4d9e-af7a-f6f6e9b6add1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311c0d0-0857-4875-a0c5-2bcd63c57e12",
   "metadata": {},
   "source": [
    "# Model Download & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d4bf15-487c-421d-9fdf-9831049eebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "WB_PROJECT_NAME = \"finetuning_demo\"\n",
    "\n",
    "LLAMA_MODEL_ID = \"meta-llama/Llama-3.1-8B\"\n",
    "FOUNDATION_SEC_8B_MODEL_ID = \"fdtn-ai/Foundation-Sec-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c79173d-4c2c-4d64-85bd-8b306086b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# tokenizer is the same for all processes\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e600bf-ffc5-42d6-bc3f-5bafcfae320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id):\n",
    "\n",
    "    # Uncomment below if you want to enable QLoRa instead of normal LoRa\n",
    "    # bnb_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit = True,\n",
    "    #     bnb_4bit_quant_type = \"nf4\",\n",
    "    #     bnb_4bit_compute_dtype = \"float16\",\n",
    "    #     bnb_4bit_use_double_quant = True\n",
    "    # )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path = model_id,\n",
    "        device_map = DEVICE,\n",
    "        # quantization_config = bnb_config,\n",
    "    ).to(DEVICE)\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.generation_config.top_p = None\n",
    "    model.generation_config.temperature = None\n",
    "    model.generation_config.pad_token_id = tokenizer.eos_token_id    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "860ad951-3df4-4cc0-9410-e9b281dc56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = \"technique: \"\n",
    "MAX_LENGTH = 256\n",
    "MAX_NEW_TOKEN = 5\n",
    "\n",
    "def inference(prompt, model):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        _output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens = MAX_NEW_TOKEN,\n",
    "            do_sample = False,\n",
    "            repetition_penalty = 1.2,\n",
    "        )\n",
    "    output = tokenizer.decode(_output[0], skip_special_tokens = True)\n",
    "    response = output.split(splitter)[-1].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb26870-baaf-46ca-b89d-13c2c4b628a1",
   "metadata": {},
   "source": [
    "Let's see how each model works with an example. \n",
    "\n",
    "Give a prompt to each model and see what the output looks like. <br>\n",
    "The correct answer is T1047. Original llama failed to answer correctly, while Foundation-Sec-8B did successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fde2e6de-b1e0-4b82-8104-8ce96eb25532",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "context: This downloader is unique per system and contains a customized backdoor written in Assembler\n",
    "technique: T1059\n",
    "\n",
    "context: This malware was capable of stealing significant system and network information\n",
    "technique: T1082\n",
    "\n",
    "context: Email phishing credential theft\n",
    "technique: T1566\n",
    "\n",
    "context: they are served a ZIP archive containing a malicious LNK file.\n",
    "technique: T1204\n",
    "\n",
    "context: download and deploy Trickbot on the user's machine\n",
    "technique: T1105\n",
    "\n",
    "context: POSHSPY's use of WMI to both store and persist the backdoor code makes it nearly invisible to anyone not familiar with the intricacies of WMI.\n",
    "technique: T'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56aa77a4-42d2-4bea-adfe-a76974a17795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ba8c7a5371461783b7b445cbfff025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1547.001\n"
     ]
    }
   ],
   "source": [
    "llama_model = load_model(LLAMA_MODEL_ID)\n",
    "llama_output_test = inference(prompt, llama_model)\n",
    "\n",
    "#To avoid OOM error load model one by one and remove models not currently being used\n",
    "import gc\n",
    "\n",
    "llama_model = None\n",
    "gc.collect()\n",
    "\n",
    "print(llama_output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5342d3fd-264e-4d4b-8403-847a734e03d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddc040ee3314ce895c03cdfd80c33d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1047\n"
     ]
    }
   ],
   "source": [
    "foundation_sec_8b_model = load_model(FOUNDATION_SEC_8B_MODEL_ID)\n",
    "foundation_sec_8b_output_test = inference(prompt, foundation_sec_8b_model)\n",
    "\n",
    "foundation_sec_8b_model = None\n",
    "gc.collect()\n",
    "\n",
    "print(foundation_sec_8b_output_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe7411-bb5e-4fa8-b8b0-384f80699424",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Let's download datasets and pre-process them for evaluation and finetuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eeaac2e-5ae3-4691-838f-577a75fe947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = '''context: {context}\n",
    "technique: T'''\n",
    "\n",
    "finetuning_prompt_template = '''context: {context}\n",
    "reason: {description}\n",
    "technique: T{label}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ea22271-d2b8-4bfe-bb52-d64d89923cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from https://github.com/dessertlab/CTI-HAL first\n",
    "# Here it's assumed that CIT-HAL is downloaded at current directory\n",
    "from pathlib import Path\n",
    "\n",
    "PATH_TO_CTI_HAL = Path(\"CTI-HAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf404cb-6413-4300-b76c-4e5d7dd368a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datasets import Dataset, load_from_disk\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def _collect_all_files(path: Path, extension: str):\n",
    "    files_names = []\n",
    "    for file_path in path.rglob(f\"*.{extension}\"):  # rglob searches recursively\n",
    "        if file_path.is_file():\n",
    "            files_names.append(file_path)\n",
    "    return files_names\n",
    "\n",
    "\n",
    "def get_attack_ids():\n",
    "    \"\"\"Get all attack IDs from the MITRE ATT&CK website.\"\"\"\n",
    "    URL = \"https://attack.mitre.org/techniques/enterprise/\"\n",
    "    page = urlopen(URL)\n",
    "    html = page.read().decode(\"utf-8\")\n",
    "    pattern = r'\\bT\\d{4}\\b'\n",
    "    matches = re.findall(pattern, html)\n",
    "    matches.sort()\n",
    "    attack_ids = set(matches)\n",
    "    attack_ids = sorted(attack_ids)\n",
    "\n",
    "    EXPECTED_NUM_ATTACK_IDS = 211\n",
    "    assert len(attack_ids) == EXPECTED_NUM_ATTACK_IDS, f\"Expected {EXPECTED_NUM_ATTACK_IDS} attack IDs (as of End of April 2025), but got {len(attack_ids)}\"\n",
    "\n",
    "    return attack_ids\n",
    "\n",
    "\n",
    "def make_datasets(path_to_datasets: Path, csv_path: str):\n",
    "    files_names = _collect_all_files(path_to_datasets, extension = \"json\")\n",
    "    rows = []\n",
    "    for json_file_name in files_names:\n",
    "        with open(json_file_name, \"r\") as f:\n",
    "            json_data = json.load(f)        \n",
    "            for item in json_data:\n",
    "                try:\n",
    "                    context = item[\"context\"]\n",
    "                    description = item[\"metadata\"][\"description\"]\n",
    "                    label = item[\"technique\"]\n",
    "                    if context == \"\" or label == None:\n",
    "                        continue\n",
    "                    rows.append([context, description, label])\n",
    "                except KeyError:\n",
    "                    print(f\"KeyError in file {json_file_name}: {item}\")\n",
    "                    continue\n",
    "\n",
    "    header = ['context', 'description', 'label']\n",
    "    with open(csv_path, 'wt') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(header)\n",
    "        for i, row in enumerate(rows):\n",
    "            try:\n",
    "                csv_writer.writerow(row)\n",
    "            except:\n",
    "                print(f\"Error writing row {i}, skipping\")\n",
    "                continue\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.drop_duplicates().set_index(\"context\")\n",
    "    attack_ids = get_attack_ids()\n",
    "    df = df[df[\"label\"].isin(attack_ids)]    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47fa4b9f-3e7d-454f-ba44-7f32cec1601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_SIZE = 0.2\n",
    "\n",
    "CSV_NAME = \"datasets.csv\"\n",
    "HF_DATASET_NAME = \"hf_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f02a7ac-d4c9-4dd8-a0bf-3ed1ac1408ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error writing row 596, skipping\n",
      "Error writing row 599, skipping\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78310ffb079747a6b895f768280b0bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6235ea0aae9f41d5b5443de5cb435e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = make_datasets(PATH_TO_CTI_HAL / \"data\", csv_path = PATH_TO_CTI_HAL / CSV_NAME)\n",
    "dataset = Dataset.from_pandas(df).train_test_split(test_size = SPLIT_SIZE, shuffle = True, seed = SEED)\n",
    "dataset.save_to_disk(PATH_TO_CTI_HAL / HF_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d80e8141-7480-4415-8599-ad8b5433e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_and_preprocess():\n",
    "\n",
    "    def _preprocess_data(examples):\n",
    "\n",
    "        assert(len(examples[\"description\"]) == len(examples[\"context\"]) == len(examples[\"label\"])), \"Length of description, context and label must be the same\"\n",
    "        total_len = len(examples[\"description\"])\n",
    "\n",
    "        prompts = [\n",
    "            finetuning_prompt_template.format(\n",
    "                description = description,\n",
    "                context = context,\n",
    "                label = label[1:] # To remove first T as they are already a part of template\n",
    "            )\n",
    "            for description, context, label in zip(\n",
    "                examples[\"description\"],\n",
    "                examples[\"context\"],\n",
    "                examples[\"label\"]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return tokenizer(prompts, truncation = True, padding = \"max_length\", max_length = MAX_LENGTH)\n",
    "\n",
    "    hf_datasets = load_from_disk(PATH_TO_CTI_HAL /  HF_DATASET_NAME)\n",
    "    train_data = hf_datasets[\"train\"]\n",
    "    test_data = hf_datasets[\"test\"]\n",
    "    tokenized_train = train_data.map(_preprocess_data, batched=True, remove_columns=[\"context\", \"description\", \"label\"])\n",
    "    tokenized_test = test_data.map(_preprocess_data, batched=True, remove_columns=[\"context\", \"description\", \"label\"])\n",
    "    print(f\"Train samples: {len(tokenized_train)}, Test samples: {len(tokenized_test)}\")\n",
    "\n",
    "    return tokenized_train, tokenized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6858eb5-b579-48a3-ae01-c1fd4624c042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e0e4aa9e8444a494cc79a364815927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca5ae42712d41d68fb280f180a243ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1247, Test samples: 312\n"
     ]
    }
   ],
   "source": [
    "tokenized_train, tokenized_test = load_dataset_and_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e2217-c7b9-4f6e-add7-00016457241c",
   "metadata": {},
   "source": [
    "# Evaluation (before finetuning)\n",
    "\n",
    "Let's see how models perform before finetuing is conducted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12a8f03b-ccf6-495c-b938-daf09e6d5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts_and_labels():\n",
    "    hf_dataset = load_from_disk(PATH_TO_CTI_HAL / HF_DATASET_NAME)\n",
    "    df = hf_dataset[\"test\"].to_pandas()\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    for row in df.iterrows():\n",
    "        row = row[1]\n",
    "        context = row['context']\n",
    "        description = row['description']\n",
    "        label = row['label']\n",
    "        prompt = prompt_template.format(description=description, context=context)\n",
    "        prompts.append(prompt)\n",
    "        labels.append(label)\n",
    "    return prompts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f50cd9e-f992-4148-84f8-ae5780096484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _reg_check(label, pred, idx):\n",
    "    pattern = r'\\bT\\d{4}\\b'\n",
    "    matches = re.findall(pattern, pred)\n",
    "    if matches and matches[idx] == label:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def evaluate_pred(prompts, labels, model):\n",
    "    preds = [inference(prompt, model) for prompt in prompts]\n",
    "    num_exist = sum(1 for label, pred in zip(labels, preds) if _reg_check(str(label), pred, 0))\n",
    "    print(f\"{num_exist} out of total {len(labels)}\")\n",
    "    return round(num_exist/len(labels), 4)\n",
    "\n",
    "def eval(model_id):\n",
    "    model = load_model(model_id)\n",
    "    prompts, labels = get_prompts_and_labels()\n",
    "    result = evaluate_pred(prompts, labels, model)\n",
    "    print(f\"Accuracy: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "245e4d53-75b1-4b20-b04c-ef8c678bdf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b99ae881e4a4482bd06a1cbbb32ac01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 out of total 312\n",
      "Accuracy: 0.1474\n"
     ]
    }
   ],
   "source": [
    "eval(LLAMA_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a5923a5-2a26-4bae-9182-0465030d853c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688f58353539414aadbd4f3c324db775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 out of total 312\n",
      "Accuracy: 0.4327\n"
     ]
    }
   ],
   "source": [
    "eval(FOUNDATION_SEC_8B_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2381e83b-dd43-4a8e-a18f-0e5ffcfd9342",
   "metadata": {},
   "source": [
    "It shows that Foundation-Sec-8B already outperforms the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab2bc5b-0c21-4eaa-b99a-8c5cd44b5d8f",
   "metadata": {},
   "source": [
    "# Finetuning as CausalML & Evaluation\n",
    "\n",
    "Let's finetune the models using LoRa approach, maintaining the model as CausalML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aee503-8db8-4144-8e37-989f3eedf3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "OUTPUT_DIR = \"./checkpoints\"\n",
    "\n",
    "def train(model_id):\n",
    "\n",
    "    _output_dir = Path(OUTPUT_DIR) / str(f\"{model_id}\").replace(\"/\", \"_\")\n",
    "\n",
    "    model = load_model(model_id)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = _output_dir,\n",
    "        label_names = [\"labels\"],\n",
    "        run_name = \"finetuning_demo\",\n",
    "        num_train_epochs = 50,\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        eval_strategy = \"no\",\n",
    "        logging_steps = 50,\n",
    "        learning_rate = 4.e-5,\n",
    "        weight_decay = 0.001,\n",
    "        fp16 = False,\n",
    "        bf16 = False,\n",
    "        max_grad_norm = 0.3,\n",
    "        max_steps = -1,\n",
    "        group_by_length = True,\n",
    "        lr_scheduler_type = \"constant\",\n",
    "        seed = SEED,\n",
    "        report_to = [\"none\"],\n",
    "    )\n",
    "    \n",
    "    peft_parameters = LoraConfig(\n",
    "        lora_alpha = 8,\n",
    "        lora_dropout = 0.1,\n",
    "        r = 8,\n",
    "        bias = \"none\",\n",
    "        task_type = \"CAUSAL_LM\",\n",
    "    )\n",
    "    peft_model = get_peft_model(model, peft_parameters)\n",
    "    peft_model.print_trainable_parameters()\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model = peft_model,\n",
    "        train_dataset = tokenized_train,\n",
    "        eval_dataset = tokenized_test,\n",
    "        peft_config = peft_parameters,\n",
    "        args = training_args,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34f15f22-039e-4166-9aea-e1b493ff6caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_finetuned_model(original_model_id):\n",
    "    _dir = Path(OUTPUT_DIR) / str(f\"{original_model_id}\").replace(\"/\", \"_\")\n",
    "    last_checkpoint = get_last_checkpoint(_dir)\n",
    "    print(\"last_checkpoint:\", last_checkpoint)\n",
    "    peft_config = PeftConfig.from_pretrained(last_checkpoint)\n",
    "    orginal_model = AutoModelForCausalLM.from_pretrained(\n",
    "        original_model_id,\n",
    "        torch_dtype = torch.float16,\n",
    "        device_map = DEVICE,\n",
    "    )\n",
    "    peft_model = PeftModel.from_pretrained(orginal_model, last_checkpoint, is_trainable=True)\n",
    "    model = peft_model.merge_and_unload()\n",
    "    model.generation_config.top_p = None\n",
    "    model.generation_config.temperature = None\n",
    "    model.generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def eval_finetuning(original_model_id):\n",
    "    model = load_finetuned_model(original_model_id)\n",
    "    prompts, labels = get_prompts_and_labels()\n",
    "    result = evaluate_pred(prompts, labels, model)\n",
    "    print(f\"metrics: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2b65146-1955-4a4f-bf67-d59eaaf43973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52c543b0eeb470e9399b0ae4f6df85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206a5eb62c4c41059fbf74a5be81d8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245579935ba04cf28f98791d7470b259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [950/950 1:08:16, Epoch 47/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.738000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.068700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.057600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(LLAMA_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbefe3bb-9acf-45a0-8c61-850be2a049b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_checkpoint: checkpoints/meta-llama_Llama-3.1-8B/checkpoint-950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1907d6ba774c4aab14802a83da1542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 out of total 312\n",
      "metrics: 0.4808\n"
     ]
    }
   ],
   "source": [
    "eval_finetuning(LLAMA_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c547d663-0a47-40d8-8dde-6d1472bb64c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7350e87bb76a46a98f00f586edcfd34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,034,717,696 || trainable%: 0.0424\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [950/950 1:08:32, Epoch 47/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.065900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.050700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.045200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(FOUNDATION_SEC_8B_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6838bc45-e34f-41f9-b60f-e73f35156049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_checkpoint: checkpoints/fdtn-ai_Foundation-Sec-8B/checkpoint-950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9ed225014c4b3db0b269bc4eb0d796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 out of total 312\n",
      "metrics: 0.5385\n"
     ]
    }
   ],
   "source": [
    "eval_finetuning(FOUNDATION_SEC_8B_MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac821b7-9a77-4646-bfe3-96b399f71ece",
   "metadata": {},
   "source": [
    "Both of performances of the original model and Foundation-Sec-8B have improved, and finetuned Foundation-Sec-8B still outperforms the finetuned llama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d65682-ba33-44ed-aed5-3a3d26c8dbe9",
   "metadata": {},
   "source": [
    "# Finetuning with Classification Head\n",
    "Another approach of finetuning the models is to replace the lm head with a classification head.\n",
    "Though this is a more popular approach for encoder models, it's also available for decoder models like Foundation-Sec-8B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ec783-c965-41f1-9dd4-5abc9121aa71",
   "metadata": {},
   "source": [
    "To load models use AutoModelForSequenceClassification instead. \n",
    "It's also necessary to map labels to indices and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30c319ea-4e93-4143-977a-9670c2574fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "def map_id_and_label():\n",
    "    \"\"\"Collect all attack IDs from the MITRE ATT&CK website and map them to labels.\"\"\"\n",
    "\n",
    "    attack_ids = get_attack_ids()\n",
    "    id2label = {}\n",
    "    label2id = {}\n",
    "    for i, attack_id in enumerate(attack_ids):\n",
    "        id2label[i] = attack_id\n",
    "        label2id[attack_id] = i\n",
    "\n",
    "    return id2label, label2id\n",
    "\n",
    "\n",
    "def load_model_with_classification_head(model_id):\n",
    "    \"\"\"Load the model from the specified path.\"\"\"\n",
    "    id2label, label2id = map_id_and_label()\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id, \n",
    "        num_labels = len(id2label), \n",
    "        id2label = id2label, \n",
    "        label2id = label2id, \n",
    "        device_map = \"auto\",\n",
    "        torch_dtype = torch.bfloat16,\n",
    "    )\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2af3e83-8450-4e64-8457-7d76edea2013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ce754e18c44355886686f7497d0057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863d59ecf3a248a4bd07e41fc9dd8294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44acfb68a0e040888fa1cc5ef938fe54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1247 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9dfae7d7bd40e1b2f6bb10158d69f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1247, Eval samples: 312\n"
     ]
    }
   ],
   "source": [
    "_, label2id = map_id_and_label()\n",
    "\n",
    "def map_labels(example):\n",
    "    \"\"\"Map the labels to their corresponding IDs.\"\"\"\n",
    "    example[\"label\"] = label2id[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "\n",
    "def load_dataset_and_preprocess_for_classification_head():\n",
    "\n",
    "    hf_datasets = load_from_disk(PATH_TO_CTI_HAL /  HF_DATASET_NAME)\n",
    "    train_data = hf_datasets[\"train\"]\n",
    "    eval_data = hf_datasets[\"test\"]\n",
    "\n",
    "    train_data = train_data.map(map_labels)\n",
    "    eval_data = eval_data.map(map_labels)\n",
    "\n",
    "    def _preprocess_data(examples):\n",
    "        return tokenizer(examples[\"context\"], truncation = True, padding = \"max_length\", max_length = MAX_LENGTH)\n",
    "\n",
    "    tokenized_train = train_data.map(_preprocess_data, batched=True, remove_columns=[\"context\", \"description\"])\n",
    "    tokenized_eval = eval_data.map(_preprocess_data, batched=True, remove_columns=[\"context\", \"description\"])\n",
    "    print(f\"Train samples: {len(tokenized_train)}, Eval samples: {len(tokenized_eval)}\")    \n",
    "\n",
    "    return tokenized_train, tokenized_eval\n",
    "\n",
    "\n",
    "tokenized_train_ch, tokenized_eval_ch = load_dataset_and_preprocess_for_classification_head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a3acd42-f459-4064-8370-729eb575b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load('recall')\n",
    "precision = evaluate.load(\"precision\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    _accuracy = accuracy.compute(predictions=predictions, references=labels)\n",
    "    _recall = recall.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    _precision = precision.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": _accuracy[\"accuracy\"],\n",
    "        \"recall\": _recall[\"recall\"],\n",
    "        \"precision\": _precision[\"precision\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4fce42-98ac-457e-a704-4823e1364861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "def train_with_classification_head(model_id):\n",
    "\n",
    "    _output_dir = Path(OUTPUT_DIR) / (str(f\"{model_id}\").replace(\"/\", \"_\") + \"_classification_head\")\n",
    "\n",
    "    model = load_model_with_classification_head(model_id)    \n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir = _output_dir,\n",
    "        learning_rate = 4e-5,\n",
    "        per_device_train_batch_size = 16,\n",
    "        per_device_eval_batch_size = 256,\n",
    "        gradient_accumulation_steps = 8,      \n",
    "        num_train_epochs = 3,\n",
    "        weight_decay = 0.01,\n",
    "        logging_steps = 3,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 3,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 42,\n",
    "        report_to = [\"none\"],\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = tokenized_train_ch,\n",
    "        eval_dataset = tokenized_eval_ch,\n",
    "        compute_metrics = compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3e2d30e-3d8e-469b-9c22-b6e218a53cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab96b41f91d743188a2f531ad42d745f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.1-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 04:56, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.531200</td>\n",
       "      <td>7.760417</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.035870</td>\n",
       "      <td>0.007730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.138000</td>\n",
       "      <td>5.779647</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.018855</td>\n",
       "      <td>0.013339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.559900</td>\n",
       "      <td>4.685096</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.061085</td>\n",
       "      <td>0.057106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.583300</td>\n",
       "      <td>3.597756</td>\n",
       "      <td>0.323718</td>\n",
       "      <td>0.133727</td>\n",
       "      <td>0.146530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.906200</td>\n",
       "      <td>3.223558</td>\n",
       "      <td>0.429487</td>\n",
       "      <td>0.170217</td>\n",
       "      <td>0.162027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.764600</td>\n",
       "      <td>2.939503</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.171264</td>\n",
       "      <td>0.149596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.048500</td>\n",
       "      <td>2.860176</td>\n",
       "      <td>0.426282</td>\n",
       "      <td>0.170450</td>\n",
       "      <td>0.153106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.789100</td>\n",
       "      <td>2.844551</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.165133</td>\n",
       "      <td>0.151304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.747700</td>\n",
       "      <td>2.844551</td>\n",
       "      <td>0.429487</td>\n",
       "      <td>0.170395</td>\n",
       "      <td>0.155997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_with_classification_head(LLAMA_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2632dcc-1240-46ff-b0a6-e41e68c88a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d5edb152ca4efd879e17cdb27d6a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at fdtn-ai/Foundation-Sec-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 04:55, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.824200</td>\n",
       "      <td>6.604968</td>\n",
       "      <td>0.099359</td>\n",
       "      <td>0.058172</td>\n",
       "      <td>0.054370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.503900</td>\n",
       "      <td>3.835737</td>\n",
       "      <td>0.301282</td>\n",
       "      <td>0.107163</td>\n",
       "      <td>0.115338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.330700</td>\n",
       "      <td>3.082532</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.163168</td>\n",
       "      <td>0.157744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.900600</td>\n",
       "      <td>2.888221</td>\n",
       "      <td>0.451923</td>\n",
       "      <td>0.199301</td>\n",
       "      <td>0.224316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.567600</td>\n",
       "      <td>2.691907</td>\n",
       "      <td>0.477564</td>\n",
       "      <td>0.209952</td>\n",
       "      <td>0.236418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.363900</td>\n",
       "      <td>2.675080</td>\n",
       "      <td>0.493590</td>\n",
       "      <td>0.213282</td>\n",
       "      <td>0.241888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.091000</td>\n",
       "      <td>2.687901</td>\n",
       "      <td>0.496795</td>\n",
       "      <td>0.213918</td>\n",
       "      <td>0.255955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.613600</td>\n",
       "      <td>2.687901</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.214362</td>\n",
       "      <td>0.256059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>2.687901</td>\n",
       "      <td>0.496795</td>\n",
       "      <td>0.213918</td>\n",
       "      <td>0.255789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_with_classification_head(FOUNDATION_SEC_8B_MODEL_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
