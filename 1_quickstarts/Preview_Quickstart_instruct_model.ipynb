{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57867395-c47c-4043-adc1-d00ac15e46b0",
   "metadata": {},
   "source": [
    "# [Preview] Quickstart (Instruct Model)\n",
    "\n",
    "This notebook demonstrates how to download Foundation AI's instruct model from Hugging Face and run an initial inference as a starting point. <br>\n",
    "If youâ€™re interested in more detailed cybersecurity [use cases](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/2_examples) or [adoptions](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/3_adoptions), please refer to the corresponding sections.\n",
    "\n",
    "## Notes\n",
    "This model is an instruction-following model fine-tuned for responding to prompted instructions. <br>\n",
    "Unlike completion model (Foundation-Sec-8B), it is designed to engage in conversations.\n",
    "\n",
    "**This model is currently in preview mode and may receive updates. As a result, outputs can vary even when parameters are configured to ensure reproducibility.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efd38b-6998-4e30-bc1f-5555212e50a2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We recommend running the scripts with NVIDIA GPU(s) for optimal performance. <br>\n",
    "While the code should work with both single and multiple GPUs, unexpected issues may arise with multiple GPUs. In such cases, minor code adjustments or limiting usage to one GPU (e.g., by setting CUDA_VISIBLE_DEVICES='0') might be necessary.\n",
    "<br> Ensure a minimum of 20 GB of available storage and memory for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b3e11c-1c28-4e56-9f97-f5fb4df4e6eb",
   "metadata": {},
   "source": [
    "## If you have an access via Hugging Face\n",
    "Since the model is in preview mode, you'll need to log in to your authorized Hugging Face account and use the correct token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6143904-42aa-4227-8094-6111773fa625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# export Huggfing Face token to HF_TOKEN\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd6fb15-3c11-4fef-b33e-d97e8e42854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9dfa982-b85a-456c-8060-0b18870f7aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1155eced54f2483b902f9b41a49df4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"\" # To be relaced with the final model name\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, # this model's tensor_type is BF16\n",
    "    token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d37ca-c2d5-46e4-b79d-aca7cbe23145",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "You can adjust the model's text generation behavior by tuning its arguments. <br>\n",
    "Below is an example configuration to ensure reproducible outputs. <br>\n",
    "For a complete list of arguments and detailed explanations, refer to the [text generation document](https://huggingface.co/docs/transformers/en/main_classes/text_generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fec24757-9aae-4aa0-af4f-3faa283c41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": None,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": False,\n",
    "    \"use_cache\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2944a2fc-0b1d-4840-a90b-108e715aef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a cybersecurity expert.\"\n",
    "\n",
    "def inference(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **generation_args,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens = False)\n",
    "    \n",
    "    # extract the assistant response part only\n",
    "    match = re.search(r\"<\\|assistant\\|>(.*?)<\\|end_of_text\\|>\", response, re.DOTALL)\n",
    "    \n",
    "    return match.group(1).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd89a39-fa13-42f7-8efc-c5482154a7d8",
   "metadata": {},
   "source": [
    "## If you have an access via remote API\n",
    "Please prepare API_KEY as well as endpoint urls provided by Foundation AI team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4849a524-f6ef-4d65-9b89-c7323ebf4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"\"\n",
    "ENDPOINT_URL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c41847-a64a-4ce6-ad21-3d02116cb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def inference(prompt):\n",
    "    data = {'prompt': prompt}\n",
    "    # If you want to add your own generation_args, you can do so like this:\n",
    "    # data['generate_args'] = YOUR_GENERATION_ARGS\n",
    "    response = requests.post(\n",
    "        ENDPOINT_URL,\n",
    "        headers={\"Authorization\": f\"Api-Key {API_KEY}\"},\n",
    "        json=data,\n",
    "    )\n",
    "\n",
    "    match = re.search(r\"<\\|assistant\\|>(.*?)<\\|end_of_text\\|>\", response.text, re.DOTALL)\n",
    "    return match.group(1).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf682b-656d-4134-9c3c-8cc7f3cb648f",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "If you want to know what MITRE ATT&CK means, you can structure the prompt as shown below. <br>\n",
    "Unlike Foundation-Sec-8B model, the model will return natural responses when you ask a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4742ae0a-da4d-41ac-b73c-18b3e9b21dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MITRE ATT&CK is a widely used framework that provides a comprehensive understanding of the tactics, techniques, and procedures (TTPs) used by cyber adversaries. It's designed to help organizations better understand and defend against various types of attacks across different levels of an enterprise network.\n"
     ]
    }
   ],
   "source": [
    "print(inference(\"What is MITRE ATT&CK? Give a very brief answer\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
