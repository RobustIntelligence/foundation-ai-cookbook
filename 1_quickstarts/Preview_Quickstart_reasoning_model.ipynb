{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d49126a-52e1-4a80-8638-e9c497732635",
   "metadata": {},
   "source": [
    "# [Preview] Quickstart (Reasoning Model)\n",
    "\n",
    "This notebook demonstrates how to download Foundation AI's reasoning model from Hugging Face and run an initial inference as a starting point. <br>\n",
    "If you’re interested in more detailed cybersecurity [use cases](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/2_examples) or [adoptions](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/3_adoptions), please refer to the corresponding sections.\n",
    "\n",
    "## Notes\n",
    "This is a reasoning model, designed to tackle complex tasks that require multi-step reasoning and explicit logical thinking. They are particularly effective for tasks like automate red-teaming, advanced incident investigation, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee91e02-a0bd-451e-95d3-1d8076b1b365",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We recommend running the scripts with NVIDIA GPU(s) for optimal performance. <br>\n",
    "While the code should work with both single and multiple GPUs, unexpected issues may arise with multiple GPUs. In such cases, minor code adjustments or limiting usage to one GPU (e.g., by setting CUDA_VISIBLE_DEVICES='0') might be necessary.\n",
    "<br> Ensure a minimum of 40 GB of available storage and memory for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d812c-96c2-40bb-a511-522ef7146917",
   "metadata": {},
   "source": [
    "## Important\n",
    "Since the model is in preview mode, you'll need to log in to your authorized Hugging Face account and use the correct token. <br> \n",
    "If you have any questions, please contact our team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e223dfb-b0cf-4e98-a6cb-fdb76252d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# export Huggfing Face token to HF_TOKEN\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8680dc-4c89-4b45-a0ca-de90814e643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1827a42-57a4-4153-a60d-02498ec6400b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b0815a63f44a3bba3c05c54ddb9a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"\" # To be relaced with the final model name\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float32, # this model's tensor_type is float32\n",
    "    token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f167842-47af-46f2-bb69-9fff403d1d18",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "You can adjust the model's text generation behavior by tuning its arguments. <br>\n",
    "Below is an example configuration to ensure reproducible outputs. <br>\n",
    "For a complete list of arguments and detailed explanations, refer to the [text generation document](https://huggingface.co/docs/transformers/en/main_classes/text_generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5fa006-4d38-4809-9ddb-8c15001599b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": None,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": False,\n",
    "    \"use_cache\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7343adc-75bf-442f-9500-d770a8ee9d02",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec20c874-235f-46c0-82e2-c71d7fbe7124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def inference(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # This <think> token is essential for reasoning model.\n",
    "    inputs += \"<think>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **generation_args,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens = False)\n",
    "\n",
    "    # extract the thinking part only\n",
    "    match = re.search(r\"<think>(.*?)<\\|end_of_text\\|>\", response, re.DOTALL)\n",
    "    \n",
    "    return match.group(1).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89b6c1-7668-4bba-bb5b-4f422c670b0d",
   "metadata": {},
   "source": [
    "Below is a simple query to request best practices for security. <br>\n",
    "The reasoning model can handle complex tasks, such as identifying vulnerabilities in security logs or triaging alerts. <br>\n",
    "For more examples, refer to [various use cases](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/2_examples) for those usages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31d0e15b-dd4f-434d-afa2-1ab62a088fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When developing Retrieval-Augmented Generation (RAG) applications, which integrate large language models with a retrieval system to provide context-aware responses, several key security considerations must be addressed. Here's an overview of best practices that can help ensure your application is secure:\n",
       "\n",
       "### 1. Data Privacy and Security\n",
       "\n",
       "- **Anonymization:** Ensure that retrieved documents do not contain sensitive information about individuals. Use anonymization techniques where necessary.\n",
       "- **Data Encryption:** Encrypt data both at rest and in transit. This includes encryption of stored documents and any communication between clients and servers.\n",
       "- **Access Control:** Implement strict access controls. Only authorized personnel should have access to retrieve or store documents within the system.\n",
       "\n",
       "### 2. Secure Data Storage and Retrieval\n",
       "\n",
       "- **Secure APIs:** When using external databases or APIs for document storage, use HTTPS and authenticate requests securely (e.g., OAuth).\n",
       "- **Indexing Sensitive Information Minimally:** Avoid indexing personal identifiable information (PII), financial details, etc., unless absolutely necessary. If such data is indexed, implement strong protections against unauthorized access.\n",
       "- **Regular Audits:** Conduct regular audits on what data is being stored and how it’s accessed. Remove unnecessary data from indexes.\n",
       "\n",
       "### 3. Model Safety and Bias Mitigation\n",
       "\n",
       "- **Bias Assessment:** Evaluate the training datasets used by LLMs for biases. The same applies to the retrieved content; ensure they don't propagate harmful stereotypes or misinformation.\n",
       "- **Safety Measures:** Deploy safety measures like prompt evaluation, output filtering, and adversarial testing to prevent generation of unsafe outputs.\n",
       "- **Model Updates:** Regularly update models to address newly discovered vulnerabilities and improve fairness and bias reduction.\n",
       "\n",
       "### 4. Input Validation and Sanitization\n",
       "\n",
       "- **Input Filtering:** Validate and sanitize inputs to prevent injection attacks (like SQLi if interacting with databases directly). For text-based inputs, this might involve removing malicious code patterns.\n",
       "- **Rate Limiting:** Apply rate limiting to prevent denial-of-service (DoS) attacks via excessive queries.\n",
       "\n",
       "### 5. Output Handling and Moderation\n",
       "\n",
       "- **Content Moderation:** Integrate robust moderation systems to filter generated outputs for toxic content, hate speech, or other unwanted materials.\n",
       "- **User Feedback Loops:** Allow users to flag inappropriate answers, helping refine future generations and retrievals.\n",
       "\n",
       "### 6. Compliance with Regulations\n",
       "\n",
       "- **GDPR/CCPA Compliance:** Adhere to privacy regulations regarding user data processing and storage. Obtain necessary consents and inform users about their rights.\n",
       "- **HIPAA Compliance** (if handling health info): Ensure all processes comply with HIPAA standards for protecting patient data.\n",
       "\n",
       "### 7. Incident Response Plan\n",
       "\n",
       "- **Monitoring:** Set up monitoring tools to detect anomalies or suspicious activities.\n",
       "- **Incident Response Team:** Have a team ready to respond quickly to breaches or leaks.\n",
       "- **Post-Incident Analysis:** After incidents, conduct thorough analyses to understand causes and improve defenses.\n",
       "\n",
       "### 8. Third-party Dependencies Management\n",
       "\n",
       "- **Vendor Risk Management:** Assess third-party services' security postures, especially those hosting or managing parts of your infrastructure or data.\n",
       "- **Up-to-date Libraries:** Keep dependencies updated to mitigate known vulnerabilities in libraries and frameworks used.\n",
       "\n",
       "By following these best practices, developers can significantly enhance the security posture of their RAG applications, ensuring safer interactions and better protection of user data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(inference(\"What are the best practices for security when developing RAG applications?\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
