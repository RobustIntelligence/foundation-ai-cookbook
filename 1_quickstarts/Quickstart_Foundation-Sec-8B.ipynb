{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa14dbad-c8cb-40e4-bc34-d297faf50eeb",
   "metadata": {},
   "source": [
    "# Quickstart (Foundation-Sec-8B)\n",
    "\n",
    "This notebook demonstrates how to download Foundation AI's base model, Foundation-Sec-8B, from Hugging Face and run an initial inference as a starting point. <br>\n",
    "If you’re interested in more detailed cybersecurity [use cases](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/2_examples) or [adoptions](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/3_adoptions) (e.g., fine-tuning), please refer to the corresponding sections.\n",
    "\n",
    "## Notes\n",
    "**Foundation-Sec-8B** is a completion model, meaning it generates additional text that is likely to follow a given prompt. The model is ideal for generating short responses (e.g., words or brief sentences) and serves as a base model for further fine-tuning. However, if you need a model to answer questions, perform reasoning, or generate full texts (e.g., security reports), it is recommended to use a reasoning model or an instruction-based model.\n",
    "\n",
    "Please also refer to [the model card](https://huggingface.co/fdtn-ai/Foundation-Sec-8B)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a41a25-aa14-49f5-a074-ba4ca6a580f5",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We recommend running the scripts with NVIDIA GPU(s) for optimal performance. <br>\n",
    "While the code should work with both single and multiple GPUs, unexpected issues may arise with multiple GPUs. In such cases, minor code adjustments or limiting usage to one GPU (e.g., by setting CUDA_VISIBLE_DEVICES='0') might be necessary.\n",
    "<br> Ensure a minimum of 20 GB of available storage and memory for the model.\n",
    "\n",
    "### Want to try models on laptops (CPUs)?\n",
    "Our quantized models are optimized to run efficiently on laptops using CPUs. <br> For detailed information and sample code, refer to [the section of quantization](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/3_adoptions/quantization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e1315df-1f47-40fe-bd33-0edcc7289189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede52103-5b2e-4768-816e-6cc6b0f54773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11c313fe42c4a3d8886193ef8f1a075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545961b575904f5fb3cddbed14e41489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d2fe29ad7a4c829404f8074a188d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/630 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55ccfcf43a64aa68e525f62f91c38ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/840 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691cc005707a42b0a4ab8bea95f0d3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109490d238b0487dbcae44063cf200b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fb87654374418eb6436ef64f681f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a0033997bce4ed3974488ddf634d0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e259a043244b5885d34b2b8d155f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0886b50c3e434a7c9dcf4b99048d3e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca43c2194084920abcf6087f09f44d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb372b87262a4ff7b58ebe5360e3aab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"fdtn-ai/Foundation-Sec-8B\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path = MODEL_ID,\n",
    "    device_map = \"auto\",\n",
    "    torch_dtype=torch.bfloat16, # Foundation-Sec-8B's tensor_type is BF16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e710bb-e183-4c96-9ec6-8191dc00400b",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "You can adjust the model's text generation behavior by tuning its arguments. <br>\n",
    "Below is an example configuration to ensure reproducible outputs. <br>\n",
    "For a complete list of arguments and detailed explanations, refer to the [text generation document](https://huggingface.co/docs/transformers/en/main_classes/text_generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaf61cc-08b9-433a-b326-b4dff14e63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": None,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": False,\n",
    "    \"use_cache\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77731d07-a359-4324-a4b0-37a5ad102689",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "To perform inference from a remote endpoint, deploy the models on a host server. <br>\n",
    "We’ve provided sample code for deploying on Amazon SageMaker AI and performing inference. Check it out [here](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/3_adoptions/deployment/sagemaker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3685d713-e4dd-4d7b-a7f5-f572bbc3ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **generation_args,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5df00",
   "metadata": {},
   "source": [
    "Our base model is supported by the Hugging Face Inference Provider (powered by[Featherless.ai](https://featherless.ai/)).  <br>\n",
    "You can run inference directly against the endpoint, eliminating the need to prepare computing resources or download the model. \n",
    "\n",
    "Note that you'll need a Hugging Face token (saved locally as `HF_TOKEN`) and will incur costs. <br>\n",
    "For more information, refer to the Hugging Face documentation: https://huggingface.co/docs/inference-providers/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbab4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args_hf_inference = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"temperature\": None,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "def hf_inference(prompt, generation_args=generation_args_hf_inference):\n",
    "    \"\"\"Run Foundation AI models using the Hugging Face inference provider (featherless-ai)\"\"\"\n",
    "\n",
    "    client = InferenceClient(\n",
    "        provider=\"featherless-ai\",\n",
    "        api_key=os.environ[\"HF_TOKEN\"],\n",
    "    )\n",
    "    \n",
    "    completion = client.text_generation(\n",
    "        model=MODEL_ID,\n",
    "        prompt=prompt,\n",
    "        **generation_args,\n",
    "    )\n",
    "    \n",
    "    return prompt + completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c35903-178a-458b-a627-effa57152851",
   "metadata": {},
   "source": [
    "If you want to know what MITRE ATT&CK means, you can structure the prompt as shown below. <br>\n",
    "It is not advisable to ask 'What is MITRE ATT&CK?' directly, as this model is not trained for answering questions and may produce unexpected responses. <br>\n",
    "If you intend to use models this way, consider using a reasoning model or an instruct model, which will be released soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b279f6-2014-49f3-843b-508a5711b31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MITRE ATT&CK is a globally accessible knowledge base of adversary tactics and techniques based on real-world observations. The goal is to help cyber defense professionals and tool providers improve their detection and response capabilities by providing them with a common taxonomy for describing how adversaries operate.\n",
      "The MITRE Corporation, which operates federally funded research centers in the United States, created this framework as part of its work supporting U.S. government cybersecurity initiatives. It was first released publicly in 2015 but has since been adopted widely across industries worldwide due to its comprehensive coverage of various attack vectors used by malicious actors today.\n"
     ]
    }
   ],
   "source": [
    "print(inference(\"MITRE ATT&CK is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f2fb3-2026-42eb-9203-ac8c93aa952c",
   "metadata": {},
   "source": [
    "### Few shots inference\n",
    "If you want the model to respond in a specific format, you can use a few-shot approach. <br>\n",
    "For instance, to match CWE IDs with CVE IDs, provide 5 example pairs of CWE and CVE, followed by the CVE ID you want to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5b0c55d-c2df-4aa9-b1f4-abaa8320c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"CVE-2021-44228 is a remote code execution flaw in Apache Log4j2 via unsafe JNDI lookups (“Log4Shell”). The CWE is CWE-502.\n",
    "\n",
    "CVE-2017-0144 is a remote code execution vulnerability in Microsoft’s SMBv1 server (“EternalBlue”) due to a buffer overflow. The CWE is CWE-119.\n",
    "\n",
    "CVE-2014-0160 is an information-disclosure bug in OpenSSL’s heartbeat extension (“Heartbleed”) causing out-of-bounds reads. The CWE is CWE-125.\n",
    "\n",
    "CVE-2017-5638 is a remote code execution issue in Apache Struts 2’s Jakarta Multipart parser stemming from improper input validation of the Content-Type header. The CWE is CWE-20.\n",
    "\n",
    "CVE-2019-0708 is a remote code execution vulnerability in Microsoft’s Remote Desktop Services (“BlueKeep”) triggered by a use-after-free. The CWE is CWE-416.\n",
    "\n",
    "CVE-2015-10011 is a vulnerability about OpenDNS OpenResolve improper log output neutralization. The CWE is\"\"\"\n",
    "\n",
    "# Update the max_new_tokens to 3, as we just want to get CWE-ID.\n",
    "generation_args[\"max_new_tokens\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0836be08-a895-47f6-bff3-e06ca8b27210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE-117\n"
     ]
    }
   ],
   "source": [
    "response = inference(prompt)\n",
    "\n",
    "# Remove the prompt part is as it's redundant\n",
    "response = response.replace(prompt, \"\").strip()\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
