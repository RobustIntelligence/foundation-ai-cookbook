{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57867395-c47c-4043-adc1-d00ac15e46b0",
   "metadata": {},
   "source": [
    "# Quickstart (Instruct Model)\n",
    "\n",
    "This notebook demonstrates how to download Foundation AI's instruct model from Hugging Face and run an initial inference as a starting point. <br>\n",
    "If you’re interested in more detailed cybersecurity [use cases](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/2_examples) or [adoptions](https://github.com/RobustIntelligence/foundation-ai-cookbook/tree/main/3_adoptions), please refer to the corresponding sections.\n",
    "\n",
    "## Notes\n",
    "This model is an instruction-following model fine-tuned for responding to prompted instructions. <br>\n",
    "Unlike completion model (Foundation-Sec-8B), it is designed to engage in conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24efd38b-6998-4e30-bc1f-5555212e50a2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We recommend running the scripts with NVIDIA GPU(s) for optimal performance. <br>\n",
    "While the code should work with both single and multiple GPUs, unexpected issues may arise with multiple GPUs. In such cases, minor code adjustments or limiting usage to one GPU (e.g., by setting CUDA_VISIBLE_DEVICES='0') might be necessary.\n",
    "<br> Ensure a minimum of 20 GB of available storage and memory for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6143904-42aa-4227-8094-6111773fa625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# export Huggfing Face token to HF_TOKEN\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd6fb15-3c11-4fef-b33e-d97e8e42854d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9dfa982-b85a-456c-8060-0b18870f7aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e794db889a4d3d95db72f23f73a543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_ID = \"fdtn-ai/Foundation-Sec-8B-Instruct\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, # this model's tensor_type is BF16\n",
    "    token=HF_TOKEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d37ca-c2d5-46e4-b79d-aca7cbe23145",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "You can adjust the model's text generation behavior by tuning its arguments. <br>\n",
    "Below is an example configuration to ensure reproducible outputs. <br>\n",
    "For a complete list of arguments and detailed explanations, refer to the [text generation document](https://huggingface.co/docs/transformers/en/main_classes/text_generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fec24757-9aae-4aa0-af4f-3faa283c41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": None,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"do_sample\": False,\n",
    "    \"use_cache\": True,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944a2fc-0b1d-4840-a90b-108e715aef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are a cybersecurity expert.\"\n",
    "# The system prompt is for demo purpose.\n",
    "# We have developed a detailed system prompt for general user interaction, which was tested\n",
    "# in internal testing and found that it improved user satisfaction and safety.\n",
    "\n",
    "# If you want to use the full system prompt, you can uncomment the line below.\n",
    "# with open(\"recommended_system_prompt_for_instruct_model.txt\", \"r\") as f:\n",
    "#     DEFAULT_SYSTEM_PROMPT = f.read()\n",
    "\n",
    "def inference(request, system_prompt = DEFAULT_SYSTEM_PROMPT):\n",
    "    \n",
    "    if isinstance(request, str):\n",
    "        messages =  [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": request},\n",
    "        ]\n",
    "    elif isinstance(request, list):\n",
    "        if request[0].get(\"role\") != \"system\":\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}] + request\n",
    "        else:\n",
    "            messages = request\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Request is not well formed. It must be a string or list of dict with correct format.\"\n",
    "        )\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            **generation_args,\n",
    "        )\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][input_ids.shape[1]:],  # Only get new tokens\n",
    "        skip_special_tokens = False\n",
    "    )\n",
    "    \n",
    "    if response.endswith(tokenizer.eos_token):\n",
    "        response = response[:-len(tokenizer.eos_token)]\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cf682b-656d-4134-9c3c-8cc7f3cb648f",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "If you want to know what MITRE ATT&CK means, you can structure the prompt as shown below. <br>\n",
    "Unlike Foundation-Sec-8B model, the model will return natural responses when you ask a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4742ae0a-da4d-41ac-b73c-18b3e9b21dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MITRE ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) is a globally accessible knowledge base of adversary tactics and techniques based on real-world observations. It's used to describe the actions that adversaries take during cyber attacks, helping organizations understand threats better for improved defense strategies and incident response planning in cybersecurity.\n"
     ]
    }
   ],
   "source": [
    "print(inference(\"What is MITRE ATT&CK? Give a very brief answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ffbf4d-b064-4017-aab9-657f018ae53a",
   "metadata": {},
   "source": [
    "## Multi-turn\n",
    "We can also convert the inference function into a multi-turn chat agent.<br>\n",
    "Below is a sample chat demo where you can enter your prompt in the blank field. You can further respond to the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d232263d-4f7e-4489-a98b-303747ea498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "class ChatApp():\n",
    "    \n",
    "    def __init__(self, system_message = DEFAULT_SYSTEM_PROMPT):\n",
    "        self.system_message = system_message\n",
    "        self.messages = [{\"role\": \"system\", \"content\": self.system_message}]\n",
    "\n",
    "    def chat(self):\n",
    "        print(\"-\" * 40)\n",
    "        print(\"Type 'quit', 'exit', or 'q' to end the conversation\")\n",
    "        print(\"Type 'clear' to clear conversation history\")\n",
    "        print(\"Type 'history' to see conversation history\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"🤖 Chat begins\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\n💬 You: \").strip()\n",
    "    \n",
    "                if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                    print(\"\\n👋 Goodbye!\")\n",
    "                    break\n",
    "                elif user_input.lower() == 'clear':\n",
    "                    self.messages = [{\"role\": \"system\", \"content\": self.system_message}]\n",
    "                    print(\"🧹 Conversation history cleared!\")\n",
    "                    continue\n",
    "                elif user_input.lower() == 'history':\n",
    "                    if self.messages:\n",
    "                        print(\"\\n==========📜 Conversation History 📜==========\")\n",
    "                        for message in self.messages:\n",
    "                            print(message.get(\"role\", \"Unknown\").title(),\":\", message.get(\"content\", \"N/A\"), \"\\n\")\n",
    "                        print(\"========== End of Conversation History ==========\")\n",
    "                    else:\n",
    "                        print(\"📜 No conversation history yet.\")\n",
    "                    continue                    \n",
    "                elif not user_input:\n",
    "                    print(\"❌ Please enter a message.\")\n",
    "                    continue\n",
    "    \n",
    "                print(\"\\n🤔 Thinking...\")\n",
    "                self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "                response = inference(self.messages)\n",
    "    \n",
    "                print(\"\\n🤖 Assistant: \")\n",
    "                display(Markdown(response))\n",
    "                self.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\n👋 Chat interrupted. Goodbye!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb349623-8136-4c9a-a296-79cfff50d94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Type 'quit', 'exit', or 'q' to end the conversation\n",
      "Type 'clear' to clear conversation history\n",
      "Type 'history' to see conversation history\n",
      "----------------------------------------\n",
      "🤖 Chat begins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 You:  What is MITRE ATT&CK? Give a very brief answer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤔 Thinking...\n",
      "\n",
      "🤖 Assistant: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "MITRE ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) is a globally accessible knowledge base of adversary tactics and techniques based on real-world observations. It's used to describe the actions that adversaries take during cyber attacks, helping organizations understand threats better for improved defense strategies and incident response planning in cybersecurity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 You:  Thank you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤔 Thinking...\n",
      "\n",
      "🤖 Assistant: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "You're welcome! If you have any more questions about cybersecurity or anything else, feel free to ask. Happy learning!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 You:  clear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Conversation history cleared!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 You:  Tell me briefly what OSINT means\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤔 Thinking...\n",
      "\n",
      "🤖 Assistant: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "OSINT stands for Open Source Intelligence. It refers to the process of collecting and analyzing data from publicly available sources, such as social media platforms, websites, forums, news articles, public records, and other open-access information repositories, in order to gather intelligence or uncover specific pieces of information relevant to an investigation, security analysis, competitive research, or any situation where knowledge is power. The goal of OSINT is to extract valuable insights without breaching privacy laws or ethical boundaries by relying solely on openly accessible resources."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 You:  Summarize in 1 sentence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤔 Thinking...\n",
      "\n",
      "🤖 Assistant: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Open Source Intelligence (OSINT) involves gathering and analyzing publicly available information from various online and offline sources to obtain actionable intelligence legally and ethically."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 You:  history\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========📜 Conversation History 📜==========\n",
      "System : You are a cybersecurity expert. \n",
      "\n",
      "User : Tell me briefly what OSINT means \n",
      "\n",
      "Assistant : OSINT stands for Open Source Intelligence. It refers to the process of collecting and analyzing data from publicly available sources, such as social media platforms, websites, forums, news articles, public records, and other open-access information repositories, in order to gather intelligence or uncover specific pieces of information relevant to an investigation, security analysis, competitive research, or any situation where knowledge is power. The goal of OSINT is to extract valuable insights without breaching privacy laws or ethical boundaries by relying solely on openly accessible resources. \n",
      "\n",
      "User : Summarize in 1 sentence \n",
      "\n",
      "Assistant : Open Source Intelligence (OSINT) involves gathering and analyzing publicly available information from various online and offline sources to obtain actionable intelligence legally and ethically. \n",
      "\n",
      "========== End of Conversation History ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "👋 Goodbye!\n"
     ]
    }
   ],
   "source": [
    "chatapp = ChatApp()\n",
    "chatapp.chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
